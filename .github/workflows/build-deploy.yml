name: Build and Deploy API

on:
  push:
    branches: [main]
    paths: 
      - 'app/**'
      - 'k8s/**'
      - '.github/workflows/build-deploy.yml'
  pull_request:
    branches: [main]
    paths:
      - 'app/**'
      - 'k8s/**'
      - '.github/workflows/build-deploy.yml'

# Required for OIDC authentication to AWS
permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: us-east-1
  ECR_REPOSITORY: liatrio-demo-dev-api
  EKS_CLUSTER_NAME: liatrio-demo-dev-eks-u58x74nr

jobs:
  build-and-push:
    runs-on: ubuntu-latest
    environment: dev
    
    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
      image-digest: ${{ steps.build.outputs.digest }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials using OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ vars.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: GitHubActions-BuildDeploy

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,format=short
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push Docker image
        id: build
        uses: docker/build-push-action@v5
        with:
          context: ./app
          platforms: linux/amd64,linux/arm64
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  deploy:
    needs: build-and-push
    runs-on: ubuntu-latest
    environment: dev
    if: github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials using OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ vars.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: GitHubActions-Deploy

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}

      - name: Deploy to EKS using AWS CLI
        run: |
          # Use the short SHA for the image tag
          IMAGE_TAG="${GITHUB_SHA:0:7}"
          IMAGE_URI="${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:${IMAGE_TAG}"
          
          # Update deployment manifest with actual image URI
          sed -i "s|IMAGE_URI_PLACEHOLDER|${IMAGE_URI}|g" k8s/deployment.yaml
          
          # Check cluster status
          echo "=== Cluster Status ==="
          aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} --query 'cluster.{Status:status,Version:version,Endpoint:endpoint}'
          
          # Check node groups
          echo "=== Node Groups ==="
          aws eks list-nodegroups --cluster-name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }}
          
          # Describe node group status
          NODEGROUP_NAME=$(aws eks list-nodegroups --cluster-name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} --query 'nodegroups[0]' --output text)
          if [ "$NODEGROUP_NAME" != "None" ]; then
            echo "=== Node Group Status ==="
            aws eks describe-nodegroup --cluster-name ${{ env.EKS_CLUSTER_NAME }} --nodegroup-name $NODEGROUP_NAME --region ${{ env.AWS_REGION }} --query 'nodegroup.{Status:status,InstanceTypes:instanceTypes,ScalingConfig:scalingConfig}'
          fi
          
          # Apply manifests (this still requires kubectl but we'll add error handling)
          echo "=== Applying Kubernetes Manifests ==="
          if kubectl apply -f k8s/deployment.yaml; then
            echo "✅ Deployment applied successfully"
          else
            echo "❌ Deployment failed - checking cluster access"
            aws sts get-caller-identity
            aws eks list-access-entries --cluster-name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} || echo "No access to list entries"
            exit 1
          fi

      - name: Verify deployment using AWS CLI
        run: |
          echo "=== Current AWS Identity ==="
          aws sts get-caller-identity
          
          echo "=== EKS Cluster Health ==="
          aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} --query 'cluster.{Status:status,Health:health,Version:version}'
          
          echo "=== Node Group Health ==="
          NODEGROUP_NAME=$(aws eks list-nodegroups --cluster-name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} --query 'nodegroups[0]' --output text)
          if [ "$NODEGROUP_NAME" != "None" ]; then
            aws eks describe-nodegroup --cluster-name ${{ env.EKS_CLUSTER_NAME }} --nodegroup-name $NODEGROUP_NAME --region ${{ env.AWS_REGION }} --query 'nodegroup.{Status:status,Health:health,Capacity:scalingConfig}'
          fi
          
          echo "=== Cluster Add-ons ==="
          aws eks list-addons --cluster-name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }}
          
          # If kubectl works, get additional info
          echo "=== Attempting kubectl verification ==="
          if kubectl get nodes --timeout=10s; then
            echo "✅ kubectl is working"
            kubectl get pods -l app=liatrio-demo-api -o wide || echo "No pods found yet"
            kubectl get services liatrio-demo-api-service || echo "Service not found yet"
          else
            echo "❌ kubectl authentication failed"
            echo "Checking EKS access entries..."
            aws eks list-access-entries --cluster-name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} || echo "Cannot list access entries"
          fi